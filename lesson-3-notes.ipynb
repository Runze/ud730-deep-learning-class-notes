{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight sharing: when you know two inputs can contain the same kind of information (i.e., statistical invariance), you share the weights $w$, and train the weights jointly for those inputs.\n",
    "- For images, this leads to convolutional networks.\n",
    "- For text and sequences in general, it leads to embeddings and recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional networks (CovNets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Link](https://www.youtube.com/watch?time_continue=65&v=ISHGyvsT0QY) to original video.\n",
    "- [Link](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/) to a very helpful blogpost explaining the concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CovNets are neural networks that share their parameters across space.\n",
    "\n",
    "Illustration of the Convolution operation (source: the blogpost linked above):\n",
    "<img src=\"https://ujwlkarn.files.wordpress.com/2016/07/convolution_schematic.gif?w=536&h=392\" alt=\"alt text\" width=\"400\">\n",
    "\n",
    "In CNN terminology,\n",
    "- the 3Ã—3 matrix is called a \"filter\" or \"kernel\" or \"feature detector\"\n",
    "- the matrix formed by sliding the filter over the image and computing the dot product is called the \"Convolved Feature\" or \"Activation Map\" or the \"Feature Map.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different values of the filter matrix will produce different Feature Maps for the same input image:\n",
    "<img src=\"https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-05-at-11-03-00-pm.png\" alt=\"alt text\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- Depth: the number of filters we use for the convolution operation, which corresponds to the number of feature maps produced.\n",
    "- Stride: the number of pixels by which we slide our filter matrix over the input matrix. When the stride is 1 then we move the filters one pixel at a time. When the stride is 2, then the filters jump 2 pixels at a time. Having a larger stride will generally produce smaller output image size (or feature maps).\n",
    "- Padding:\n",
    "    - Valid padding: not passing the edge\n",
    "    - Same padding: going off the edge and pad with 0s so that the output image size is the same as the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea of CovNets is to apply convolutions that are going to progressively *squeeze out* the spacial dimensions while increasing the depth which corresponds roughly to the semantic complexity of your representation. As all this spacial information has been squeezed out, only parameters that map to the content of the image remain.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Runze/ud730-deep-learning-class-notes/master/screenshots/lesson-4-covnets.png\" alt=\"alt text\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "\n",
    "Pooling reduces the dimensionality of each feature map but retains the most important information.\n",
    "\n",
    "<img src=\"https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-10-at-3-38-39-am.png?w=988\" alt=\"alt text\" width=\"400\">\n",
    "\n",
    "A small network with two convolutional layers, followed by one fully connected layer:\n",
    "\n",
    "<img src=\"https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-08-at-2-26-09-am.png?w=1496\" alt=\"alt text\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception module\n",
    "\n",
    "<img src=\"https://github.com/Runze/ud730-deep-learning-class-notes/blob/master/screenshots/lesson-4-inception.png?raw=true\" alt=\"alt text\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
